package com.scistor.label.hive;import com.scistor.label.common.MessageException;import iie.udps.common.hcatalog.SerHCatOutputFormat;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hive.conf.HiveConf;import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;import org.apache.hadoop.hive.metastore.api.FieldSchema;import org.apache.hadoop.hive.metastore.api.SerDeInfo;import org.apache.hadoop.hive.metastore.api.StorageDescriptor;import org.apache.hadoop.hive.metastore.api.Table;import org.apache.hadoop.hive.ql.io.RCFileInputFormat;import org.apache.hadoop.hive.ql.io.RCFileOutputFormat;import org.apache.hadoop.hive.serde.serdeConstants;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.mapreduce.Job;import org.apache.hive.hcatalog.common.HCatUtil;import org.apache.hive.hcatalog.data.HCatRecord;import org.apache.hive.hcatalog.data.schema.HCatSchema;import org.apache.hive.hcatalog.mapreduce.OutputJobInfo;import org.apache.spark.SerializableWritable;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.PairFunction;import org.apache.thrift.TException;import scala.Tuple2;import java.io.IOException;import java.util.HashMap;import java.util.List;import java.util.Map;/** * @ClassName: HCatOutputTable * @Description: 代表Hive中尚未存在的表，用于存储数据 * @Author: wutianjie * @CreateDate: 2015.08.10 * @UpdateDate: 2015.08.13 * @UpdateRemark: 添加列选择功能及注释 * @Version: 0.1 */public class HCatOutputTable extends HCatTable {    public HCatOutputTable(String metaStoreURI, String databaseName, String tableName) {        super(metaStoreURI, databaseName, tableName);    }    public HCatOutputTable(String hCatTable) {        super(hCatTable);    }    // 将包含HCatRecord的RDD按指定schema存入新创建的Hive表中    public void write(JavaPairRDD<WritableComparable, SerializableWritable<HCatRecord>> rdd, HCatSchema hCatSchema) {        createTable(this.databaseName, this.tableName, hCatSchema);        Job outputJob = null;        try {            outputJob = Job.getInstance();            outputJob.setOutputFormatClass(SerHCatOutputFormat.class);            outputJob.setOutputKeyClass(WritableComparable.class);            outputJob.setOutputValueClass(SerializableWritable.class);            SerHCatOutputFormat.setOutput(outputJob,                    OutputJobInfo.create(this.databaseName, this.tableName, null));            SerHCatOutputFormat.setSchema(outputJob, hCatSchema);        } catch (IOException e) {            throw new MessageException("HCat write failed", e);        }        rdd.saveAsNewAPIHadoopDataset(outputJob.getConfiguration());    }    // 将包含HCatRecord的RDD按指定schema存入新创建的Hive表中    public void write(JavaRDD<SerializableWritable<HCatRecord>> rdd, HCatSchema hCatSchema) {        JavaPairRDD<WritableComparable, SerializableWritable<HCatRecord>> finalRdd = rdd                .mapToPair(new MapToSerWriHCatRecord());        write(finalRdd, hCatSchema);    }    //根据指定表模式创建Hive表    private void createTable(String dbName, String tblName, HCatSchema schema) {        HiveMetaStoreClient client = null;        try {            HiveConf hiveConf = HCatUtil.getHiveConf(new Configuration());            client = HCatUtil.getHiveClient(hiveConf);        } catch (Exception e) {            throw new MessageException("createTable failed", e);        }        try {            if (client.tableExists(dbName, tblName)) {                client.dropTable(dbName, tblName);            }        } catch (TException e) {            e.printStackTrace();        }        List<FieldSchema> fields = HCatUtil.getFieldSchemaList(schema.getFields());        Table table = new Table();        table.setDbName(dbName);        table.setTableName(tblName);        StorageDescriptor sd = new StorageDescriptor();        sd.setCols(fields);        table.setSd(sd);        sd.setInputFormat(RCFileInputFormat.class.getName());        sd.setOutputFormat(RCFileOutputFormat.class.getName());        sd.setParameters(new HashMap<String, String>());        sd.setSerdeInfo(new SerDeInfo());        sd.getSerdeInfo().setName(table.getTableName());        sd.getSerdeInfo().setParameters(new HashMap<String, String>());        sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT, "1");        sd.getSerdeInfo().setSerializationLib(org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.class.getName());        Map<String, String> tableParams = new HashMap<String, String>();        table.setParameters(tableParams);        try {            client.createTable(table);        } catch (TException e) {            throw new MessageException("HCat createTable failed", e);        } finally {            client.close();        }    }    /**     * @ClassName: MapToSerWriHCatRecord     * @Description: 将SerializableWritable转换为SerializableWritable<HCatRecord>     * @Author: wutianjie     * @CreateDate: 2015.08.13     * @UpdateDate: 2015.08.13     * @UpdateRemark: 解决HCatOutputTable调动时任务序列化问题     * @Version: 0.1     */    public static class MapToSerWriHCatRecord implements PairFunction<SerializableWritable<HCatRecord>, WritableComparable, SerializableWritable<HCatRecord>> {        public Tuple2<WritableComparable, SerializableWritable<HCatRecord>> call(                SerializableWritable<HCatRecord> record) throws Exception {            return new Tuple2<WritableComparable, SerializableWritable<HCatRecord>>(NullWritable.get(), record);        }    }}